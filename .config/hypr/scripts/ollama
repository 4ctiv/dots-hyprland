#!/bin/bash
# Author: 4ctiv
# Version: 2025-2
HELP_MSG='Help page for '$0'
Usage: '$(echo $0 | sed 's|.*/|./|g')' [flag] [model]

|----------------|
| Avalable Flags |
|----|-----------------------------------------|
| -h | --help       ~ Display this message     |
| -b | --background ~ run ollama in background |
| -d | --detatch                 "             |
| -k | --kill       ~ stop ollama container    |
| -s | --stop       ~            "             |
| -w | --web        ~ start open-webui         |
|----|-----------------------------------------|

If no flags are specified ollama will run in terminal
'

MODEL="gemma3:4b-it-qat" # qwen3-vl:2b # https://ollama.com/library
CONTAINER_NET="ollama"
CONTAINER_NAME="ollama"
PORT_PREFIX="127.0.0.1:" # Only for docker with root -> will limit access to ip
KEEP_NETWORK_ACCESS=""   # Empty -> Only network access while downloading models

if [[ $(docker info -f "{{println .SecurityOptions}}" | grep -o rootless) ]]; then
  # Docker rootless
  PORT_PREFIX=""  # unset port prefix as it would lead to errors in rootless mode
  OLLAMA_MODELS_HOST_DIR="$HOME/.ollama/models" # Per user (docker rootless)
  mkdir -p "${OLLAMA_MODELS_HOST_DIR:?no set}"
else
  # Docker as root
  PORT_PREFIX="127.0.0.1:"  # Only for docker with root -> will limit access to ip
  OLLAMA_MODELS_HOST_DIR="/opt/ollama/models"   # System wide
fi

###############
### Cleanup ###
###############
# On forced exit always disconnect ollama networks and stop container
trap '
 docker network disconnect bridge ${CONTAINER_NAME:=ollama} >/dev/null;
 docker stop ${CONTAINER_NAME:=ollama} >/dev/null 2>/dev/null ;
 docker stop open-webui   &>/dev/null ;
 exit 0' SIGINT

docker ps >/dev/null 2>/dev/null
if [ ! $? -eq 0 ]; then
  echo "Please start docker first"
  exit 3
fi
#################
### Functions ###
#################

function open-webui_stop {
  docker stop open-webui
}

function open-webui_start {
  if [[ ! -z "$(docker ps --format '{{.Names}}' | grep -o open-webui)" ]]; then
    # Already running
    return 2;
  fi
  # Setup container network
  if [[ -n "$(docker network ls --format '{{.Name}}' | grep -o ollama)" ]];then
    docker network create --driver=bridge open-webui > /dev/null 2>/dev/null
  fi
  # Update local image
  docker pull ghcr.io/open-webui/open-webui:main >/dev/null 2>/dev/null

  if [[ -n "$(docker ps -a --format '{{.Names}}' | grep -o 'open-webui')" ]]; then
    # Container exists
    docker start open-webui >/dev/null
  else
    # Create container
    docker run -d \
             -p ${PORT_PREFIX}3000:8080 \
             -v open-webui:/app/backend/data \
             --name open-webui \
             --network open-webui \
             --network ${CONTAINER_NET:=ollama} \
             -e OLLAMA_BASE_URL="http://${CONTAINER_NET:=ollama}.ollama:11434" \
             -e WEBUI_AUTH="false" \
             -e ENABLE_IMAGE_GENERATION=True \
             -e ENABLE_OPENAI_API=False \
             -e ENABLE_OLLAMA_API=True \
             ghcr.io/open-webui/open-webui:main >/dev/null
  fi &&\
    (timeout 500 bash -c 'while [ "$(curl -s -o /dev/null -w "%{http_code}" "http://localhost:3000?temporary-chat=true")" != "200" ];do sleep 1s; done') &&\
    xdg-open http://localhost:3000?temporary-chat=true& >/dev/null 2>/dev/null # opens webpage
  # echo "" # Fix prompt not returning to user input
}

function ollama_start {

  if [ ! -z "$(docker ps --format '{{.Names}}' | grep -o -E '[O|o]llama')" ];then
    # Already running
    return 2;
  fi

  # Network (internal communication)
  docker network create --internal ${CONTAINER_NET:=ollama} > /dev/null

  # Volume (ollama config & models)
  mkdir -p ${OLLAMA_MODELS_HOST_DIR:?not set} && [[ "$1" != "-q" ]] && echo "Created ${OLLAMA_MODELS_HOST_DIR}"
  docker volume create \
    --driver local \
    --opt type=none \
    --opt device="${OLLAMA_MODELS_HOST_DIR:-'./ollama/models'}" \
    --opt o=bind \
    ${CONTAINER_NAME:=ollama} >/dev/null

  # Check if ollama container already exists
  if [ -z "$(docker ps -a --format '{{.Names}}' | grep -o -E '[O|o]llama')" ]; then
    # - inxi -G  # list graphical devices (human readable)
    if [[ $(lsmod | grep nvidia) ]]; then
      # NVIDIA Graphics
      # -> https://hub.docker.com/r/ollama/ollama#nvidia
      sudo nvidia-ctk runtime configure --runtime=docker
      sudo systemctl restart docker
      printf "[INFO] Using NVIDIA GPU for running ollama\n"
      docker run -d \
           --name ${CONTAINER_NAME:=ollama} \
           --hostname ${CONTAINER_NAME:=ollama} \
           --restart=unless-stopped \
           --device=/dev/dri \
           --gpus=all \
           --network ${CONTAINER_NET:=ollama} \
           -v ollama:/root/.ollama/models \
           -p ${PORT_PREFIX}11434:11434 \
           -e PATH=/llm/ollama:$PATH \
           -e OLLAMA_HOST=0.0.0.0 \
           -e no_proxy=localhost,127.0.0.1 \
           ollama/ollama
    elif [[ $(lsmod | grep amdgpu) ]]; then
      # AMD Graphics
      # -> https://hub.docker.com/r/ollama/ollama#amd
      printf "[INFO] Using AMD GPU for running ollama\n"
      docker run -d \
           --name ${CONTAINER_NAME:=ollama} \
           --hostname ${CONTAINER_NAME:=ollama} \
           --restart=unless-stopped \
           --device=/dev/dri \
           --device /dev/kfd \
           --gpus=all \
           --network ${CONTAINER_NET:=ollama} \
           -v ollama:/root/.ollama/models \
           -p ${PORT_PREFIX}11434:11434 \
           -e PATH=/llm/ollama:$PATH \
           -e OLLAMA_HOST=0.0.0.0 \
           -e no_proxy=localhost,127.0.0.1 \
           ollama/ollama:rocm
    elif [[ $(lsmod | grep i915) ]] && [[ ! -z $(lsmod | grep VGA | grep [Aa][Rr][Cc]) ]]; then
      # Intel Graphics (ARC)
      # -> https://dev.to/itlackey/run-ollama-on-intel-arc-gpu-ipex-4e4k
      printf "[INFO] Using INTEL GPU for running ollama\n"
      docker run -d \
           --name ${CONTAINER_NAME:=ollama} \
           --hostname ${CONTAINER_NAME:=ollama} \
           --restart=unless-stopped \
           --device=/dev/dri \
           --gpus=all \
           --network ${CONTAINER_NET:=ollama} \
           -v ollama:/root/.ollama/models \
           -p ${PORT_PREFIX}11434:11434 \
           -e PATH=/llm/ollama:$PATH \
           -e OLLAMA_HOST=0.0.0.0 \
           -e no_proxy=localhost,127.0.0.1 \
           -e ZES_ENABLE_SYSMAN=1 \
           -e OLLAMA_INTEL_GPU=true \
           -e ONEAPI_DEVICE_SELECTOR=level_zero:0 \
           -e DEVICE=Arc \
           --shm-size="16g" \
           --memory="32G" \
           intelanalytics/ipex-llm-inference-cpp-xpu:latest \
           bash -c "cd /llm/scripts/ && source ipex-llm-init --gpu --device Arc && bash /llm/scripts/start-ollama.sh && tail -f /llm/ollama/ollama.log" >/dev/null
    else
      # Unknown GPU (may fall back to cpu)
      printf "\033[0;33m[WARNING]\033[0m Unknown GPU: Using CPU for running ollama\n"
      lspci -k | grep -EA3 'VGA' # shows some gpu information
      docker run -d \
        --name ${CONTAINER_NAME:=ollama} \
        --hostname ${CONTAINER_NAME:=ollama} \
        --restart=unless-stopped \
        --gpus all \
        --device=/dev/dri \
        --network ${CONTAINER_NET:=ollama} \
        -v ollama:/root/.ollama/models \
        -p ${PORT_PREFIX}11434:11434 \
        -e OLLAMA_HOST=0.0.0.0 \
        -e no_proxy=localhost,127.0.0.1 \
        ollama/ollama:latest
    fi
  fi

  # Start ollama if no already running
  [[ "$1" != "-q" ]] && echo "Starting ollama"
  docker start ${CONTAINER_NAME:=ollama} >/dev/null 2>/dev/null &&\
  docker network disconnect bridge ${CONTAINER_NAME:=ollama} >/dev/null # remove container networks (no network access)
  sleep 3s

  # Start gui for easy access
  # see: https://chatboxai.app
  if sleep 3 && curl -s -f 'http://127.0.0.1:11434/api/status' > /dev/null; then
    # Note: This will only work if ollama port is accessable on host
    # To access ollama eather give ollama network access via bridge or remap port via
    # '-v 127.0.0.1:$OLAMMA_PORT:$OLAMMA_PORT'
    if [ -f "$(which chatbox)" ]; then
      nohup chatbox > /dev/null 2>&1 &
    else
      curl -o /opt/chabox/chatbox.appimage 'https://chatboxai.app/install_chatbox/linux' &&\
      ln -s /opt/chabox/chatbox.appimage /bin/chatbox &&\
      /opt/chabox/chatbox.appimage;
    fi
  fi
}

function ollama_pull {
  # Pull ollama model if required
  [[ "$1" != "-q" ]] && echo "Reciving ollama $MODEL (ollam has temporary internet access)"
  docker network connect bridge ${CONTAINER_NAME:=ollama} >/dev/null 2>/dev/null # give container network access
  [[ "$1" != "-q" ]] && docker exec -it ${CONTAINER_NAME:=ollama} ollama pull "$MODEL" >/dev/null || docker exec -it ${CONTAINER_NAME:=ollama} ollama pull "$MODEL" # >/dev/null
  if [[ -z "$KEEP_NETWORK_ACCESS" ]]; then
    docker network disconnect bridge ${CONTAINER_NAME:=ollama} >/dev/null # remove container network access
  fi
 #docker network disconnect ${CONTAINER_NET:=ollama} ${CONTAINER_NAME:=ollama} >/dev/null # remove container network access
  [[ "$1" != "-q" ]] && echo "Fineshed reciving ollama $MODEL (temporary network access removed)"
}

function ollama_rm {
  # Remove ollama model
  docker exec -it ${CONTAINER_NAME:=ollama} ollama rm "${1:?no model specified}";
  return $?;
}

function ollama_run {
  # Run ollama model
  [[ "$1" != "-q" ]] && echo "Starting ollama $MODEL"
  docker exec -it ${CONTAINER_NAME:=ollama} ollama run "$MODEL" # repeating ollama is not a typo
}

function ollama_stop {
  # Shutdown ollama
  [[ "$1" != "-q" ]] && echo "Stopping ollama container"
  docker stop ${CONTAINER_NAME:=ollama} >/dev/null 2>/dev/null
  #docker container rm ollama > /dev/null 2>/dev/null
}

#################
### MAIN CODE ###
#################

if [ ! -z "$1" ]; then
  if [ ! -z "$2" ]; then
    MODEL="$(echo "$2" | awk '{print $NF}')"
  fi

  case "$1" in
  "-h" | "--help")
    printf "$HELP_MSG"
    exit 0
  ;;
  "-k" | "--kill"    | "-s" | "--stop")
    echo "Killing ollama & open-webui"
    docker network disconnect bridge ${CONTAINER_NAME:=ollama} >/dev/null 2>/dev/null
    docker stop ${CONTAINER_NAME:=ollama} 2>/dev/null
    docker stop open-webui 2>/dev/null
    exit $?
  ;;
  "-d" | "--detatch" | "-b" | "--background")
    echo "Starting ollama in background"
    KEEP_NETWORK_ACCESS="$2"
    ollama_start
    ollama_pull
    exit 0
  ;;
  "-w" | "--web" | "-g" | "--gui")
    echo "Starting open-webui"
    ollama_start;
    open-webui_start;
    exit 0;
  ;;
  "-p"|"--pull")
    echo "Downloading model '$2'"
    tmp=ollama_start;
    ollama_pull;
    if [ "$tmp" = "2" ]; then ollama_stop; fi
    unset tmp;
    exit 0
  ;;
  "-r"|"--remove")
    echo "Removing model '$2'";
    ollama_rm "$2";
    exit 0;
    ;;
  "*") # Expect model was given
    echo "Starting ollama with user specified model: $1"
    MODEL="$(echo "$1" | awk '{print $NF}')"
    ollama_start
    open-webui_start
    ollama_pull
    ollama_run
    ollama_stop
    exit 0
  ;;
  esac
else
  echo "Running ollama in forground (terminal) using default model"
  ollama_start
  open-webui_start
  ollama_pull
  ollama_run
  ollama_stop
  exit 0
fi
exit 1

# Image generataion setup ConfyUI + Ollama + Intel ARC
# yay -S intel-compute-runtime && lsmod | grep -i xe
# python -m venv .venv
# pip install comfy-cli
# comfy --workspace=./comfy install
# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/xpu
# comfy model download --url https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned.safetensors --relative-path models/checkpoints
# comfy launch --background -- --listen 0.0.0.0 --port 8080
# OpenWebUI > Admin Pannel > Images > Select ConfyUI > URL: http://127.0.0.1:8080
