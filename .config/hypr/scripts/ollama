#!/bin/bash
# Author: 4ctiv
# Version: 2025-2
HELP_MSG='Help page for '$0'
Usage: '$(echo $0 | sed 's|.*/|./|g')' [flag] [model]

|----------------|
| Avalable Flags |
|----|-----------------------------------------|
| -h | --help       ~ Display this message     |
| -b | --background ~ run ollama in background |
| -d | --detatch    ~ exposed ollama in bg     |
| -k | --kill       ~ stop ollama container    |
| -s | --stop       ~            "             |
| -p | --pull       ~ download a model         |
| -r | --remove     ~ remove   a model         |
| -w | --web        ~ start open-webui         |
| -u | --update     ~ update ollama version    |
|----|-----------------------------------------|

If no flags are specified ollama will run in terminal
'

# https://ollama.com/library
MODEL="ministral-3" # "gemma3:4b-it-qat" v "qwen3-vl:2b" 
CONTAINER_NET="ollama"
CONTAINER_NAME="ollama"
PORT_PREFIX="127.0.0.1:" # Only for docker with root -> will limit access to ip
KEEP_NETWORK_ACCESS=""   # Empty -> Only network access while downloading models

if [[ $(docker info -f "{{println .SecurityOptions}}" | grep -o rootless) ]]; then
  # Docker rootless
  PORT_PREFIX=""  # unset port prefix as it would lead to errors in rootless mode
  OLLAMA_MODELS_HOST_DIR="$HOME/.ollama/models" # Per user (docker rootless)
  mkdir -p "${OLLAMA_MODELS_HOST_DIR:?no set}"
else
  # Docker as root
  PORT_PREFIX="127.0.0.1:"  # Only for docker with root -> will limit access to ip
  OLLAMA_MODELS_HOST_DIR="/opt/ollama/models"   # System wide
fi

###############
### Cleanup ###
###############
# On forced exit always disconnect ollama networks and stop container
trap '
 docker network disconnect bridge ${CONTAINER_NAME:=ollama} >/dev/null;
 docker stop ${CONTAINER_NAME:=ollama} >/dev/null 2>/dev/null ;
 docker stop open-webui   &>/dev/null ;
 exit 0' SIGINT

docker ps >/dev/null 2>/dev/null
if [ ! $? -eq 0 ]; then
  echo "Please start docker first"
  exit 3
fi
#################
### Functions ###
#################

function comfy-ui_start {
  mkdir -p ~/.comfy-ui

  # https://github.com/YanWenKun/ComfyUI-Docker?tab=readme-ov-file#more-image-tags
  docker run -it -d \
  --name comfy-ui \
  --device=/dev/dri \
  -p 8188:8188 \
  -v ~/.comfy-ui/models:/root/ComfyUI/models \
  -v ~/.comfy-ui/models/hf-hub:/root/.cache/huggingface/hub \
  -v ~/.comfy-ui/models/torch-hub:/root/.cache/torch/hub \
  -v ~/.comfy-ui/input:/root/ComfyUI/input \
  -v ~/.comfy-ui/output:/root/ComfyUI/output \
  -v ~/.comfy-ui/workflows:/root/ComfyUI/user/default/workflows \
  -e CLI_ARGS="--enable-manager --disable-smart-memory --async-offload" \
  yanwk/comfyui-boot:xpu >/dev/null

  (timeout 500 bash -c 'while [ "$(curl -s -o /dev/null -w "%{http_code}" "http://${PORT_PREFIX:-localhost:}${PORT_COMFYUI:-8188}")" != "200" ];do sleep 1s; done') &&\
    xdg-open http://${PORT_PREFIX:-localhost:}${PORT_COMFYUI:-8188}& >/dev/null 2>/dev/null # opens webpage
}

function comfy-ui_stop {
  echo "Stopping comfy-ui container"
  docker stop comfy-ui >/dev/null
}

function open-webui_stop {
  echo "Stopping open-webui container"
  docker stop open-webui >/dev/null
}

function open-webui_start {
  if [[ ! -z "$(docker ps --format '{{.Names}}' | grep -o open-webui)" ]]; then
    # Already running
    return 2;
  fi

  # Setup container network
  if [[ -z "$(docker network ls --format '{{.Name}}' | grep -o 'open-webui')" ]];then
    docker network create --driver=bridge open-webui > /dev/null 2>/dev/null
  fi

  # Update local image
  docker pull ghcr.io/open-webui/open-webui:main >/dev/null 2>/dev/null

  if [[ -n "$(docker ps -a --format '{{.Names}}' | grep -o 'open-webui')" ]]; then
    # Container exists
    docker start open-webui >/dev/null
  else
    # Create container
    docker run -d \
             -p ${PORT_PREFIX}3000:8080 \
             -v open-webui:/app/backend/data \
             --name open-webui \
             --restart unless-stopped \
             --network open-webui \
             --network ${CONTAINER_NET:=ollama} \
             -e OLLAMA_BASE_URL="http://${CONTAINER_NET:=ollama}.ollama:11434" \
             -e WEBUI_AUTH="false" \
             -e ENABLE_IMAGE_GENERATION=True \
             -e ENABLE_OPENAI_API=False \
             -e ENABLE_OLLAMA_API=True \
             ghcr.io/open-webui/open-webui:main >/dev/null
  fi &&\
    (timeout 500 bash -c 'while [ "$(curl -s -o /dev/null -w "%{http_code}" "http://localhost:3000?temporary-chat=true")" != "200" ];do sleep 1s; done') &&\
    xdg-open http://localhost:3000?temporary-chat=true& >/dev/null 2>/dev/null # opens webpage

  sleep "0.5" && echo "" # Fix prompt not returning to user input
}

function ollama_start {

  if [[ -n "$(docker ps --format '{{.Names}}' | grep -o -E '[O|o]llama')" ]];then
    # Already running
    return 2;
  fi

  # Network (internal communication)
  if [[ -z "$(docker network ls --format '{{.Name}}' | grep -o '${CONTAINER_NET:=ollama}')" ]];then
    docker network create --internal ${CONTAINER_NET:=ollama} > /dev/null
  fi

  # Volume (ollama config & models)
  mkdir -p ${OLLAMA_MODELS_HOST_DIR:?not set} && echo "Modes saved to ${OLLAMA_MODELS_HOST_DIR}"
  if [ ! -z "$(docker volume ls --format '{{.Name}}' | grep -o -E '[O|o]llama')" ];then
    docker volume create \
      --opt o=bind  \
      --driver local \
      --opt type=none \
      --opt device="${OLLAMA_MODELS_HOST_DIR:-'./ollama/models'}" \
      ${CONTAINER_NAME:=ollama} >/dev/null
  fi

  # Check if ollama container already exists
  if [ -z "$(docker ps -a --format '{{.Names}}' | grep -o -E '[O|o]llama')" ]; then
    # - inxi -G  # list graphical devices (human readable)
    if [[ $(lsmod | grep nvidia) ]]; then
      # NVIDIA Graphics
      # -> https://hub.docker.com/r/ollama/ollama#nvidia
      sudo nvidia-ctk runtime configure --runtime=docker
      sudo systemctl restart docker
      printf "[INFO] Using NVIDIA GPU for running ollama\n"
      docker run -d \
           --name ${CONTAINER_NAME:=ollama} \
           --hostname ${CONTAINER_NAME:=ollama} \
           --restart=unless-stopped \
           --device=/dev/dri \
           --gpus=all \
           --network ${CONTAINER_NET:=ollama} \
           -v ollama:/root/.ollama/models \
           -p ${PORT_PREFIX}11434:11434 \
           -e PATH=/llm/ollama:$PATH \
           -e OLLAMA_HOST=0.0.0.0 \
           -e no_proxy=localhost,127.0.0.1 \
           ollama/ollama
    elif [[ $(lsmod | grep amdgpu) ]]; then
      # AMD Graphics
      # -> https://hub.docker.com/r/ollama/ollama#amd
      printf "[INFO] Using AMD GPU for running ollama\n"
      docker run -d \
           --name ${CONTAINER_NAME:=ollama} \
           --hostname ${CONTAINER_NAME:=ollama} \
           --restart=unless-stopped \
           --device=/dev/dri \
           --device /dev/kfd \
           --gpus=all \
           --network ${CONTAINER_NET:=ollama} \
           -v ollama:/root/.ollama/models \
           -p ${PORT_PREFIX}11434:11434 \
           -e PATH=/llm/ollama:$PATH \
           -e OLLAMA_HOST=0.0.0.0 \
           -e no_proxy=localhost,127.0.0.1 \
           ollama/ollama:rocm
    elif [[ $(lsmod | grep -E "(i915|xe)") ]] && [[ ! -z $(lspci -k | grep -EA3 'VGA|3D|Display' | grep [Aa][Rr][Cc]) ]]; then
      # Intel Graphics (ARC)
      # -> https://dev.to/itlackey/run-ollama-on-intel-arc-gpu-ipex-4e4k
      printf "[INFO] Using INTEL GPU for running ollama\n"
      docker run -d \
           --name ${CONTAINER_NAME:=ollama} \
           --hostname ${CONTAINER_NAME:=ollama} \
           --restart=unless-stopped \
           --device=/dev/dri \
           --network ${CONTAINER_NET:=ollama} \
           -v ollama:/root/.ollama/models \
           -p ${PORT_PREFIX}11434:11434 \
           -e PATH=/llm/ollama:$PATH \
           -e OLLAMA_HOST=0.0.0.0 \
           -e no_proxy=localhost,127.0.0.1 \
           -e ZES_ENABLE_SYSMAN=1 \
           -e OLLAMA_INTEL_GPU=true \
           -e ONEAPI_DEVICE_SELECTOR=level_zero:0 \
           -e DEVICE=Arc \
           --shm-size="16g" \
           --memory="32G" \
           intelanalytics/ipex-llm-inference-cpp-xpu:latest \
           bash -c "cd /llm/scripts/ && source ipex-llm-init --gpu --device Arc && bash /llm/scripts/start-ollama.sh && tail -f /llm/ollama/ollama.log" >/dev/null
    else
      # Unknown GPU (may fall back to cpu)
      printf "\033[0;33m[WARNING]\033[0m Unknown GPU: Using CPU for running ollama\n"
      lspci -k | grep -EA3 'VGA' # shows some gpu information
      # NOTE: --gpus all may fail if no dedicated gpu is present
      docker run -d \
        --name ${CONTAINER_NAME:=ollama} \
        --hostname ${CONTAINER_NAME:=ollama} \
        --restart=unless-stopped \
        --gpus all \
        --device=/dev/dri \
        --network ${CONTAINER_NET:=ollama} \
        -v ollama:/root/.ollama/models \
        -p ${PORT_PREFIX}11434:11434 \
        -e OLLAMA_HOST=0.0.0.0 \
        -e no_proxy=localhost,127.0.0.1 \
        ollama/ollama:latest 2>/dev/null || (\
          docker rm ollama >/dev/null && \\
          docker run -d \
            --name ${CONTAINER_NAME:=ollama} \
            --hostname ${CONTAINER_NAME:=ollama} \
            --restart=unless-stopped \
            --device=/dev/dri \
            --network ${CONTAINER_NET:=ollama} \
            -v ollama:/root/.ollama/models \
            -p ${PORT_PREFIX}11434:11434 \
            -e OLLAMA_HOST=0.0.0.0 \
            -e no_proxy=localhost,127.0.0.1 \
            ollama/ollama:latest
        )
    fi
  else
    docker start ${CONTAINER_NAME:=ollama} >/dev/null 2>/dev/null
  fi

  if [[ "$1" != "--keep-net" ]]; then
    docker network disconnect bridge ${CONTAINER_NAME:=ollama} >/dev/null 2>/dev/null # remove container networks (no network access)
  else
    echo "[INFO] Ollama port exposed on '11434' (${PORT_PREFIX})"
    docker network connect bridge ${CONTAINER_NAME:=ollama} >/dev/null 2>/dev/null    # Make container port public accessable
  fi
  return 0
}

function ollama_pull {
  # Pull ollama model if required
  if [[ -n "$(docker ps --format '{{.Names}}' | grep -o open-webui)" ]]; then
    ollama_start;
  fi
  echo "Reciving ollama $MODEL (ollam has temporary internet access)"
  docker network connect bridge ${CONTAINER_NAME:=ollama} >/dev/null 2>/dev/null # give container network access
  docker exec -it ${CONTAINER_NAME:=ollama} ollama pull "$MODEL" >/dev/null || docker exec -it ${CONTAINER_NAME:=ollama} ollama pull "$MODEL" # >/dev/null
  if [[ -z "$KEEP_NETWORK_ACCESS" ]]; then
    docker network disconnect bridge ${CONTAINER_NAME:=ollama} >/dev/null # remove container network access
  fi
 #docker network disconnect ${CONTAINER_NET:=ollama} ${CONTAINER_NAME:=ollama} >/dev/null # remove container network access
  echo "Fineshed reciving ollama $MODEL (temporary network access removed)"
}

function ollama_rm {
  # Remove ollama model
  docker exec -it ${CONTAINER_NAME:=ollama} ollama rm "${1:?no model specified}";
  return $?;
}

function ollama_update {
  docker network connect bridge ${CONTAINER_NAME:=ollama} >/dev/null 2>/dev/null # give container network access
  docker exec -it ${CONTAINER_NAME:=ollama} bash -c 'curl -fsSL https://ollama.com/install.sh | sh'
  docker exec -it ${CONTAINER_NAME:=ollama} bash -c '[ -f /usr/local/bin/ollama ] && ln -sf /usr/local/bin/ollama /llm/ollama/ollama'
  docker exec -it ${CONTAINER_NAME:=ollama} bash -c '[ -f /usr/local/bin/ollama ] && sed -i "s@^[ \t]*lib_dir=.*@lib_dir=/usr/local/bin/ollama@" /usr/local/bin/init-ollama'
  #docker exec -it ${CONTAINER_NAME:=ollama} bash -c '[ -f /usr/local/bin/ollama ] && sed -i "s/ollama,//" /usr/local/bin/init-ollama'
  docker stop  ollama
  docker start ollama
  if [[ -z "$KEEP_NETWORK_ACCESS" ]]; then
    docker network disconnect bridge ${CONTAINER_NAME:=ollama} >/dev/null # remove container network access
  fi
  echo "Fineshed updating ollama"
}

function ollama_run {
  # Run ollama model
  echo "Starting ollama $MODEL"
  if [[ -n "$@" ]];then
    docker exec -it ${CONTAINER_NAME:=ollama} ollama run "$MODEL" # repeating ollama is not a typo
  else
    docker exec -it ${CONTAINER_NAME:=ollama} ollama $@ # Run given ollama commands
  fi
}

function ollama_stop {
  # Shutdown ollama
  echo "Stopping ollama container"
  docker stop ${CONTAINER_NAME:=ollama} >/dev/null 2>/dev/null
  #docker container rm ollama > /dev/null 2>/dev/null
}

#################
### MAIN CODE ###
#################

if [ ! -z "$1" ]; then
  if [ ! -z "$2" ]; then
    MODEL="$(echo "$2" | awk '{print $NF}')"
  fi

  case "$1" in
  "-h" | "--help")
    printf "$HELP_MSG"
    exit 0
  ;;
  "-k" | "--kill"    | "-s" | "--stop")
    echo "Killing ollama & open-webui"
    docker network disconnect bridge ${CONTAINER_NAME:=ollama} >/dev/null 2>/dev/null
    docker stop ${CONTAINER_NAME:=ollama} 2>/dev/null
    docker stop open-webui 2>/dev/null
    exit $?
  ;;
  "-d" | "--detatch")
    echo "Starting ollama in background"
    KEEP_NETWORK_ACCESS="$2"
    ollama_start
    ollama_pull
    exit 0
  ;;
  "-b" | "--background")
    ollama_start --keep-net
    ollama_pull
   #open-webui_start
  ;;
  "-w" | "--web" | "-g" | "--gui")
    echo "Starting open-webui"
    ollama_start;
    open-webui_start;
    exit 0;
  ;;
  "-p"|"--pull")
    echo "Downloading model '$2'"
    tmp=ollama_start;
    ollama_pull $2;
    if [ "$tmp" = "2" ]; then ollama_stop; fi
    unset tmp;
    exit 0
  ;;
  "-r"|"--remove")
    echo "Removing model '$2'";
    ollama_rm "$2";
    exit 0;
    ;;
  "-u"|"--update")
    echo "Updating ollama";
    ollama_update;
    exit 0;
    ;;
  "*") # Expect model was given
    echo "Starting ollama with user specified model: $1"
    MODEL="$(echo "$1" | awk '{print $NF}')"
    ollama_start
    open-webui_start
    ollama_pull
    ollama_run
    ollama_stop
    exit 0
  ;;
  esac
else
  echo "Running ollama in forground (terminal,web) using default model"
  ollama_start
  open-webui_start
  ollama_pull
  ollama_run
  echo "" # Fix promt look
  ollama_stop
  open-webui_stop
  exit 0
fi
exit 1

# Image generataion setup ConfyUI + Ollama + Intel ARC 
# -> Docker setup: https://github.com/YanWenKun/ComfyUI-Docker
# -> Usage:        https://unsloth.ai/docs/models/qwen-image-2512
# -> Model:        https://huggingface.co/unsloth/Qwen-Image-Edit-2511-GGUF/resolve/main/qwen-image-edit-2511-Q3_K_M.gguf
#
# yay -S intel-compute-runtime && lsmod | grep -i xe
# python -m venv .venv
# pip install comfy-cli
# comfy --workspace=./comfy install
# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/xpu
# comfy model download --url https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned.safetensors --relative-path models/checkpoints
# comfy launch --background -- --listen 0.0.0.0 --port 8080
# OpenWebUI > Admin Pannel > Images > Select ConfyUI > URL: http://127.0.0.1:8080
